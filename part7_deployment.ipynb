{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ee8630c",
   "metadata": {},
   "source": [
    "# Part 7: Deployment\n",
    "\n",
    "In the previous sections we've seen how to train a Neural Network with a small resource footprint using QKeras, then to convert it to `hls4ml` and create an IP. That IP can be interfaced into a larger design to deploy on an FPGA device. In this section, we introduce the `VivadoAccelerator` backend of `hls4ml`, where we can easily target some supported devices to get up and running quickly. Specifically, we'll deploy the model on a [pynq-z2 board](http://www.pynq.io/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62a80f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from qkeras.utils import _add_supported_quantized_objects\n",
    "co = {}\n",
    "_add_supported_quantized_objects(co)\n",
    "import os\n",
    "os.environ['PATH'] = '/opt/Xilinx/Vivado/2019.2/bin:' + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c75d93",
   "metadata": {},
   "source": [
    "## Load model\n",
    "Load the model from `part4: quantization` (note you need to have trained the model in part 4 first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800575f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model_3/KERAS_check_best_model.h5', custom_objects=co)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce14d31",
   "metadata": {},
   "source": [
    "## Convert to hls4ml\n",
    "We'll convert our model into `hls4ml`, with a few small changes compared to the previous use of the same model in part 4.\n",
    "We target  `backend='VivadoAccelerator'` backend: this will wrap the HLS NN model, providing an AXI-Stream interface to our IP. We also specify `board='pynq-z2'`.\n",
    "\n",
    "The pynq-z2 board is a popular board with a Zynq 7020 SoC. Since this device is much smaller than the Alveo we specified in previous sections, we set the `ReuseFactor` of all the `Dense` layers of the model to 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a76ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hls4ml\n",
    "import plotting\n",
    "hls4ml.model.optimizer.OutputRoundingSaturationMode.layers = ['Activation']\n",
    "hls4ml.model.optimizer.OutputRoundingSaturationMode.rounding_mode = 'AP_RND'\n",
    "hls4ml.model.optimizer.OutputRoundingSaturationMode.saturation_mode = 'AP_SAT'\n",
    "\n",
    "config = hls4ml.utils.config_from_keras_model(model, granularity='name')\n",
    "config['LayerName']['softmax']['exp_table_t'] = 'ap_fixed<18,8>'\n",
    "config['LayerName']['softmax']['inv_table_t'] = 'ap_fixed<18,4>'\n",
    "for layer in ['fc1', 'fc2', 'fc3', 'output']:\n",
    "    config['LayerName'][layer]['ReuseFactor'] = 64\n",
    "print(\"-----------------------------------\")\n",
    "plotting.print_dict(config)\n",
    "print(\"-----------------------------------\")\n",
    "hls_model = hls4ml.converters.convert_from_keras_model(model,\n",
    "                                                       hls_config=config,\n",
    "                                                       output_dir='model_3/hls4ml_prj_pynq',\n",
    "                                                       backend='VivadoAccelerator',\n",
    "                                                       board='pynq-z2')\n",
    "hls_model.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e67d4f",
   "metadata": {},
   "source": [
    "We can query which other boards are currently supported (we're working to add more). The `VivadoAccelerator` backend introduces the `AcceleratorConfig` section of configuration. Here we can change some details of the interface to the accelerator IP.\n",
    "\n",
    "\n",
    "The `create_initial_config` method (of any backend) can be used to create a template dictionary with the default parameters that you can use as a starting point. In the conversion above, we didn't change any of these settings so all the defaults are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dbd354",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hls4ml.templates.get_supported_boards_dict().keys())\n",
    "plotting.print_dict(hls4ml.templates.get_backend('VivadoAccelerator').create_initial_config())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4bf01d",
   "metadata": {},
   "source": [
    "## Predict\n",
    "Run the CPU emulation of the hls4ml NN and save the file to compare against the hardware result later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267ecbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_test = np.load('X_test.npy')\n",
    "y_hls = hls_model.predict(np.ascontiguousarray(X_test))\n",
    "np.save('model_3/y_hls.npy', y_hls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca4f0e2",
   "metadata": {},
   "source": [
    "## Synthesize\n",
    "Now synthesize the model, and also export the IP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6c817f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_model.build(csim=False, export=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3412fa7c",
   "metadata": {},
   "source": [
    "## Make bitfile\n",
    "Now we've exported the NN IP, let's create a bitfile! The `VivadoAccelerator` backend design scripts create a Block Design in Vivado IPI containing our Neural Network IP, as well as the other necessary IPs to create a complete system.\n",
    "\n",
    "In the case of our `pynq-z2`, we add a DMA IP to transfer data between the PS and PL containg the Neural Network. If you want to create a different design, for example to connect your NN to a sensor, you can use our block designs as a starting point and add in relevant IP for your use case.\n",
    "\n",
    "Our block diagram looks like this:\n",
    "\n",
    "<img src=\"images/part7_block_design.png\" alt=\"Block Design\" style=\"width; 400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555b1243",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls4ml.templates.VivadoAcceleratorBackend.make_bitfile(hls_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd13389",
   "metadata": {},
   "source": [
    "The floorplan of our NN placed on the `pynq-z2` is shown below, with the hls4ml Neural Network highlighted in purple. You can reproduce this yourself if running the tutorial with a local Vivado installation by opening the project at `model_3/hls4ml_prj_pynq/myproject_vivado_accelerator/project_1.xpr` in the Vivado GUI and clicking \"Open Implemented Design\".\n",
    "<img src=\"images/part7_floorplan.png\" alt=\"Floorplan\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d55bac",
   "metadata": {},
   "source": [
    "Let's also inspect the final resource usage after placement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00c1adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -n '30,45p' model_3/hls4ml_prj_pynq/myproject_vivado_accelerator/project_1.runs/impl_1/design_1_wrapper_utilization_placed.rpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033cc4d9",
   "metadata": {},
   "source": [
    "## Part 7b: running on a pynq-z2\n",
    "The following section is the code to execute in the pynq-z2 jupyter notebook to execute NN inference. \n",
    "\n",
    "First, you'll need to follow the setup instructions for the pynq-z2 board, then transfer the following files from the earlier part of this notebook into a directory on the pynq-z2:\n",
    "- bitfile: `model_3/hls4ml_prj_pynq/myproject_vivado_accelerator/project_1.runs/impl_1/design_1_wrapper.bit` -> `hls4ml_nn.bit`\n",
    "- hardware handoff: `model_3/hls4ml_prj_pynq/myproject_vivado_accelerator/project_1.srcs/sources_1/bd/design_1/hw_handoff/design_1.hwh` -> `hls4ml_nn.hwh`\n",
    "- driver: `model_3/hls4ml_prj_pynq/axi_stream_driver.py` -> `axi_stream_driver.py`\n",
    "- data: `X_test.npy`, `y_test.npy`\n",
    "\n",
    "The following commands archive these files into `model_3/hls4ml_prj_pynq/package.tar.gz` that can be copied over to the pynq-z2 and extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22892f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir model_3/hls4ml_prj_pynq/package/\n",
    "!cp model_3/hls4ml_prj_pynq/myproject_vivado_accelerator/project_1.runs/impl_1/design_1_wrapper.bit model_3/hls4ml_prj_pynq/package/hls4ml_nn.bit\n",
    "!cp model_3/hls4ml_prj_pynq/myproject_vivado_accelerator/project_1.srcs/sources_1/bd/design_1/hw_handoff/design_1.hwh model_3/hls4ml_prj_pynq/package/hls4ml_nn.hwh\n",
    "!cp model_3/hls4ml_prj_pynq/axi_stream_driver.py model_3/hls4ml_prj_pynq/package/\n",
    "!cp X_test.npy y_test.npy model_3/hls4ml_prj_pynq/package\n",
    "!tar -czvf model_3/hls4ml_prj_pynq/package.tar.gz -C model_3/hls4ml_prj_pynq/package/ ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a52cfb",
   "metadata": {},
   "source": [
    "The following cells are intended to run on a pynq-z2, they will not run on the server used to train and synthesize models!\n",
    "\n",
    "First, import our driver `Overlay` class. We'll also load the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c67e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from axi_stream_driver import NeuralNetworkOverlay\n",
    "import numpy as np\n",
    "X_test = np.load('X_test.npy')\n",
    "y_test = np.load('y_test.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551c5cd6",
   "metadata": {},
   "source": [
    "Create a `NeuralNetworkOverlay` object. This will download the `Overlay` (bitfile) onto the PL of the pynq-z2. We provide the `X_test.shape` and `y_test.shape` to allocate some buffers for the data transfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb786f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetworkOverlay('hls4ml_nn.bit', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fde9b2d",
   "metadata": {},
   "source": [
    "Now run the prediction! When we set `profile=True` the function times the inference, and prints out a summary as well as returning the profiling information. We also save the output to a file so we can do some validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd6dee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hw, latency, throughput = nn.predict(X_test, profile=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1983e7d7",
   "metadata": {},
   "source": [
    "An example print out looks like:\n",
    "\n",
    "Classified 166000 samples in 0.402568 seconds (412352.6956936468 inferences / s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005ae126",
   "metadata": {},
   "source": [
    "## Part 7c: final validation\n",
    "We executed NN inference on the pynq-z2! Now we can copy the `y_hw.npy` back to the host we've been using for the training and synthesis, and make a final plot to check that the output we took on the board is as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee790be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_hw = np.load('y_hw.npy')\n",
    "y_test = np.load('y_test.npy')\n",
    "classes = np.load('classes.npy', allow_pickle=True)\n",
    "y_qkeras = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy QKeras, CPU:     {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_qkeras, axis=1))))\n",
    "print(\"Accuracy hls4ml, pynq-z2: {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_hw, axis=1))))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 9))\n",
    "_ = plotting.makeRoc(y_test, y_qkeras, classes, linestyle='-')\n",
    "plt.gca().set_prop_cycle(None) # reset the colors\n",
    "_ = plotting.makeRoc(y_test, y_hw, classes, linestyle='--')\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "lines = [Line2D([0], [0], ls='-'),\n",
    "         Line2D([0], [0], ls='--')]\n",
    "from matplotlib.legend import Legend\n",
    "leg = Legend(ax, lines, labels=['QKeras, CPU', 'hls4ml, pynq-z2'],\n",
    "            loc='lower right', frameon=False)\n",
    "ax.add_artist(leg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
